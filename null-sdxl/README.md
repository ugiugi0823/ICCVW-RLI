## Optimizing Stable Diffusion XL with Residual Interpolation Connections for Null-Text Inversion

## Abstract
This paper addresses optimization challenges encountered in Stable Diffusion XL during the null-text inversion process. A critical analysis of Stable Diffusion architectures reveals significant variations in U-Net parameters and context dimensions across versions, with the XL version featuring substantially larger scales in both aspects. The expanded attention layers in the XL architecture introduce notable optimization difficulties when implementing conventional Null-Text Inversion methods. To address these limitations, we propose Residual Linear Interpolation connections, systematically applied to self-attention and cross-attention layers within the U-Net's down blocks, up blocks, and mid blocks. Our methodology effectively mitigates gradient vanishing phenomena and stabilizes the training process. Through empirical evaluation, we demonstrate that our linear interpolation approach successfully optimizes the inversion process, yielding substantial improvements in image quality. This work contributes to the advancement of large-scale diffusion models by establishing a robust framework for text-guided image manipulation.
